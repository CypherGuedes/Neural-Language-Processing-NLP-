{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ed6f4d3-d1fc-4f1a-9981-e3855415e535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.18.0-cp312-cp312-win_amd64.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\tagsa\\anaconda3\\lib\\site-packages (4.66.4)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\tagsa\\anaconda3\\lib\\site-packages (0.8.0)\n",
      "Collecting tensorflow-intel==2.18.0 (from tensorflow)\n",
      "  Downloading tensorflow_intel-2.18.0-cp312-cp312-win_amd64.whl.metadata (4.9 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading libclang-18.1.1-py2.py3-none-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\tagsa\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\tagsa\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.20.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\tagsa\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.32.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\tagsa\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (69.5.1)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\tagsa\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading termcolor-2.5.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\tagsa\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\tagsa\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.14.1)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading grpcio-1.70.0-cp312-cp312-win_amd64.whl.metadata (4.0 kB)\n",
      "Collecting tensorboard<2.19,>=2.18 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting keras>=3.5.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading keras-3.8.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in c:\\users\\tagsa\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\tagsa\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.11.0)\n",
      "Collecting ml-dtypes<0.5.0,>=0.4.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading ml_dtypes-0.4.1-cp312-cp312-win_amd64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\tagsa\\anaconda3\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\tagsa\\anaconda3\\lib\\site-packages (from tiktoken) (2023.10.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\tagsa\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\tagsa\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\tagsa\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tagsa\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2024.8.30)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\tagsa\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.18.0->tensorflow) (0.43.0)\n",
      "Requirement already satisfied: rich in c:\\users\\tagsa\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (13.3.5)\n",
      "Collecting namex (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading namex-0.0.8-py3-none-any.whl.metadata (246 bytes)\n",
      "Collecting optree (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading optree-0.14.0-cp312-cp312-win_amd64.whl.metadata (48 kB)\n",
      "     ---------------------------------------- 0.0/48.6 kB ? eta -:--:--\n",
      "     -------------------------------- ----- 41.0/48.6 kB 960.0 kB/s eta 0:00:01\n",
      "     -------------------------------------- 48.6/48.6 kB 815.7 kB/s eta 0:00:00\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\tagsa\\anaconda3\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.4.1)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\tagsa\\anaconda3\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\tagsa\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\users\\tagsa\\anaconda3\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\tagsa\\anaconda3\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\tagsa\\anaconda3\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.1.0)\n",
      "Downloading tensorflow-2.18.0-cp312-cp312-win_amd64.whl (7.5 kB)\n",
      "Downloading tensorflow_intel-2.18.0-cp312-cp312-win_amd64.whl (390.3 MB)\n",
      "   ---------------------------------------- 0.0/390.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/390.3 MB 2.3 MB/s eta 0:02:49\n",
      "   ---------------------------------------- 0.1/390.3 MB 1.4 MB/s eta 0:04:31\n",
      "   ---------------------------------------- 0.3/390.3 MB 2.0 MB/s eta 0:03:19\n",
      "   ---------------------------------------- 0.5/390.3 MB 3.0 MB/s eta 0:02:11\n",
      "   ---------------------------------------- 1.0/390.3 MB 4.4 MB/s eta 0:01:29\n",
      "   ---------------------------------------- 1.5/390.3 MB 5.5 MB/s eta 0:01:11\n",
      "   ---------------------------------------- 2.0/390.3 MB 6.3 MB/s eta 0:01:02\n",
      "   ---------------------------------------- 2.5/390.3 MB 6.9 MB/s eta 0:00:57\n",
      "   ---------------------------------------- 3.1/390.3 MB 7.7 MB/s eta 0:00:51\n",
      "   ---------------------------------------- 3.6/390.3 MB 8.0 MB/s eta 0:00:49\n",
      "   ---------------------------------------- 4.2/390.3 MB 8.4 MB/s eta 0:00:46\n",
      "    --------------------------------------- 5.0/390.3 MB 9.2 MB/s eta 0:00:42\n",
      "    --------------------------------------- 5.7/390.3 MB 9.6 MB/s eta 0:00:40\n",
      "    --------------------------------------- 6.5/390.3 MB 10.1 MB/s eta 0:00:38\n",
      "    --------------------------------------- 7.3/390.3 MB 10.6 MB/s eta 0:00:37\n",
      "    --------------------------------------- 8.1/390.3 MB 11.0 MB/s eta 0:00:35\n",
      "    --------------------------------------- 9.0/390.3 MB 11.5 MB/s eta 0:00:34\n",
      "   - -------------------------------------- 9.9/390.3 MB 12.0 MB/s eta 0:00:32\n",
      "   - -------------------------------------- 11.0/390.3 MB 15.2 MB/s eta 0:00:25\n",
      "   - -------------------------------------- 12.2/390.3 MB 16.8 MB/s eta 0:00:23\n",
      "   - -------------------------------------- 13.4/390.3 MB 18.7 MB/s eta 0:00:21\n",
      "   - -------------------------------------- 14.6/390.3 MB 21.1 MB/s eta 0:00:18\n",
      "   - -------------------------------------- 15.8/390.3 MB 21.8 MB/s eta 0:00:18\n",
      "   - -------------------------------------- 17.1/390.3 MB 24.2 MB/s eta 0:00:16\n",
      "   - -------------------------------------- 18.6/390.3 MB 26.2 MB/s eta 0:00:15\n",
      "   -- ------------------------------------- 20.1/390.3 MB 27.3 MB/s eta 0:00:14\n",
      "   -- ------------------------------------- 21.6/390.3 MB 29.7 MB/s eta 0:00:13\n",
      "   -- ------------------------------------- 23.4/390.3 MB 31.2 MB/s eta 0:00:12\n",
      "   -- ------------------------------------- 25.1/390.3 MB 32.8 MB/s eta 0:00:12\n",
      "   -- ------------------------------------- 26.8/390.3 MB 34.4 MB/s eta 0:00:11\n",
      "   -- ------------------------------------- 28.4/390.3 MB 36.4 MB/s eta 0:00:10\n",
      "   --- ------------------------------------ 30.0/390.3 MB 36.4 MB/s eta 0:00:10\n",
      "   --- ------------------------------------ 32.0/390.3 MB 38.5 MB/s eta 0:00:10\n",
      "   --- ------------------------------------ 34.1/390.3 MB 38.5 MB/s eta 0:00:10\n",
      "   --- ------------------------------------ 36.5/390.3 MB 40.9 MB/s eta 0:00:09\n",
      "   --- ------------------------------------ 38.9/390.3 MB 46.9 MB/s eta 0:00:08\n",
      "   ---- ----------------------------------- 41.3/390.3 MB 50.4 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 43.8/390.3 MB 50.4 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 46.7/390.3 MB 54.7 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 49.2/390.3 MB 59.5 MB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 51.6/390.3 MB 54.4 MB/s eta 0:00:07\n",
      "   ----- ---------------------------------- 54.7/390.3 MB 59.5 MB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 57.4/390.3 MB 59.5 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 60.0/390.3 MB 59.8 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 63.2/390.3 MB 65.6 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 66.8/390.3 MB 65.6 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 70.1/390.3 MB 72.6 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 73.3/390.3 MB 65.6 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 77.1/390.3 MB 72.6 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 79.9/390.3 MB 72.6 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 83.4/390.3 MB 72.6 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 87.0/390.3 MB 73.1 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 91.1/390.3 MB 81.8 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 91.9/390.3 MB 59.5 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 94.9/390.3 MB 65.6 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 99.4/390.3 MB 65.6 MB/s eta 0:00:05\n",
      "   ---------- ---------------------------- 103.5/390.3 MB 81.8 MB/s eta 0:00:04\n",
      "   ---------- ---------------------------- 107.6/390.3 MB 93.0 MB/s eta 0:00:04\n",
      "   ----------- --------------------------- 111.4/390.3 MB 93.0 MB/s eta 0:00:03\n",
      "   ----------- --------------------------- 115.5/390.3 MB 81.8 MB/s eta 0:00:04\n",
      "   ----------- --------------------------- 117.7/390.3 MB 73.1 MB/s eta 0:00:04\n",
      "   ------------ -------------------------- 121.8/390.3 MB 73.1 MB/s eta 0:00:04\n",
      "   ------------ -------------------------- 123.3/390.3 MB 72.6 MB/s eta 0:00:04\n",
      "   ------------ -------------------------- 124.3/390.3 MB 50.4 MB/s eta 0:00:06\n",
      "   ------------ -------------------------- 128.2/390.3 MB 54.4 MB/s eta 0:00:05\n",
      "   ------------- ------------------------ 134.8/390.3 MB 108.8 MB/s eta 0:00:03\n",
      "   ------------- ------------------------- 137.5/390.3 MB 93.0 MB/s eta 0:00:03\n",
      "   -------------- ------------------------ 141.6/390.3 MB 81.8 MB/s eta 0:00:04\n",
      "   -------------- ------------------------ 145.3/390.3 MB 73.1 MB/s eta 0:00:04\n",
      "   -------------- ------------------------ 148.7/390.3 MB 73.1 MB/s eta 0:00:04\n",
      "   --------------- ----------------------- 152.5/390.3 MB 81.8 MB/s eta 0:00:03\n",
      "   --------------- ----------------------- 156.3/390.3 MB 81.8 MB/s eta 0:00:03\n",
      "   --------------- ----------------------- 159.7/390.3 MB 81.8 MB/s eta 0:00:03\n",
      "   ---------------- ---------------------- 163.5/390.3 MB 81.8 MB/s eta 0:00:03\n",
      "   ---------------- ---------------------- 167.5/390.3 MB 81.8 MB/s eta 0:00:03\n",
      "   ----------------- --------------------- 170.5/390.3 MB 81.8 MB/s eta 0:00:03\n",
      "   ----------------- --------------------- 174.3/390.3 MB 81.8 MB/s eta 0:00:03\n",
      "   ----------------- --------------------- 178.2/390.3 MB 81.8 MB/s eta 0:00:03\n",
      "   ------------------ -------------------- 182.1/390.3 MB 81.8 MB/s eta 0:00:03\n",
      "   ------------------ -------------------- 186.0/390.3 MB 81.8 MB/s eta 0:00:03\n",
      "   ------------------ -------------------- 188.4/390.3 MB 81.8 MB/s eta 0:00:03\n",
      "   ------------------- ------------------- 192.6/390.3 MB 72.6 MB/s eta 0:00:03\n",
      "   ------------------- ------------------- 196.7/390.3 MB 81.8 MB/s eta 0:00:03\n",
      "   -------------------- ------------------ 200.5/390.3 MB 81.8 MB/s eta 0:00:03\n",
      "   -------------------- ------------------ 204.5/390.3 MB 93.0 MB/s eta 0:00:02\n",
      "   -------------------- ------------------ 207.4/390.3 MB 81.8 MB/s eta 0:00:03\n",
      "   --------------------- ----------------- 211.2/390.3 MB 81.8 MB/s eta 0:00:03\n",
      "   --------------------- ----------------- 215.3/390.3 MB 81.8 MB/s eta 0:00:03\n",
      "   --------------------- ----------------- 217.5/390.3 MB 73.1 MB/s eta 0:00:03\n",
      "   ---------------------- ---------------- 221.3/390.3 MB 72.6 MB/s eta 0:00:03\n",
      "   ---------------------- ---------------- 225.4/390.3 MB 72.6 MB/s eta 0:00:03\n",
      "   ---------------------- ---------------- 226.3/390.3 MB 59.5 MB/s eta 0:00:03\n",
      "   ---------------------- ---------------- 227.3/390.3 MB 54.4 MB/s eta 0:00:03\n",
      "   ----------------------- --------------- 230.4/390.3 MB 50.1 MB/s eta 0:00:04\n",
      "   ----------------------- --------------- 235.3/390.3 MB 54.7 MB/s eta 0:00:03\n",
      "   ----------------------- -------------- 240.9/390.3 MB 131.2 MB/s eta 0:00:02\n",
      "   ------------------------ -------------- 244.9/390.3 MB 93.9 MB/s eta 0:00:02\n",
      "   ------------------------ -------------- 248.8/390.3 MB 81.8 MB/s eta 0:00:02\n",
      "   ------------------------- ------------- 252.8/390.3 MB 81.8 MB/s eta 0:00:02\n",
      "   ------------------------- ------------- 256.4/390.3 MB 81.8 MB/s eta 0:00:02\n",
      "   ------------------------- ------------- 260.1/390.3 MB 72.6 MB/s eta 0:00:02\n",
      "   -------------------------- ------------ 263.9/390.3 MB 81.8 MB/s eta 0:00:02\n",
      "   -------------------------- ------------ 266.4/390.3 MB 81.8 MB/s eta 0:00:02\n",
      "   --------------------------- ----------- 271.2/390.3 MB 73.1 MB/s eta 0:00:02\n",
      "   --------------------------- ----------- 274.9/390.3 MB 73.1 MB/s eta 0:00:02\n",
      "   --------------------------- ----------- 278.8/390.3 MB 93.9 MB/s eta 0:00:02\n",
      "   ---------------------------- ---------- 282.8/390.3 MB 93.9 MB/s eta 0:00:02\n",
      "   ---------------------------- ---------- 286.1/390.3 MB 81.8 MB/s eta 0:00:02\n",
      "   ---------------------------- ---------- 289.8/390.3 MB 81.8 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 293.8/390.3 MB 81.8 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 297.5/390.3 MB 93.0 MB/s eta 0:00:01\n",
      "   ------------------------------ -------- 301.3/390.3 MB 93.0 MB/s eta 0:00:01\n",
      "   ------------------------------ -------- 305.3/390.3 MB 81.8 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 308.7/390.3 MB 81.8 MB/s eta 0:00:01\n",
      "   ------------------------------- ------- 312.8/390.3 MB 81.8 MB/s eta 0:00:01\n",
      "   ------------------------------- ------- 312.8/390.3 MB 81.8 MB/s eta 0:00:01\n",
      "   ------------------------------- ------- 318.2/390.3 MB 72.6 MB/s eta 0:00:01\n",
      "   -------------------------------- ------ 320.9/390.3 MB 65.6 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 323.9/390.3 MB 93.0 MB/s eta 0:00:01\n",
      "   -------------------------------- ------ 327.8/390.3 MB 72.6 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 330.6/390.3 MB 72.6 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 334.2/390.3 MB 81.8 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 337.1/390.3 MB 73.1 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 340.2/390.3 MB 73.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 343.8/390.3 MB 72.6 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 346.6/390.3 MB 65.6 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 350.2/390.3 MB 72.6 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 352.9/390.3 MB 65.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 356.6/390.3 MB 72.6 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 359.0/390.3 MB 65.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 362.4/390.3 MB 65.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 364.8/390.3 MB 65.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ -- 368.6/390.3 MB 65.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 372.6/390.3 MB 72.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 374.7/390.3 MB 73.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 378.3/390.3 MB 65.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  381.0/390.3 MB 65.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  384.8/390.3 MB 65.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  387.9/390.3 MB 65.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.3/390.3 MB 72.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.3/390.3 MB 72.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.3/390.3 MB 72.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.3/390.3 MB 72.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.3/390.3 MB 72.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.3/390.3 MB 72.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.3/390.3 MB 72.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.3/390.3 MB 72.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.3/390.3 MB 72.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.3/390.3 MB 72.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.3/390.3 MB 72.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.3/390.3 MB 72.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.3/390.3 MB 72.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.3/390.3 MB 72.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.3/390.3 MB 72.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.3/390.3 MB 72.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.3/390.3 MB 72.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.3/390.3 MB 72.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.3/390.3 MB 72.6 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.3/390.3 MB 72.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 390.3/390.3 MB 9.5 MB/s eta 0:00:00\n",
      "Downloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "   ---------------------------------------- 0.0/133.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 133.7/133.7 kB 8.2 MB/s eta 0:00:00\n",
      "Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Downloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "   ---------------------------------------- 0.0/57.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 57.5/57.5 kB 3.1 MB/s eta 0:00:00\n",
      "Downloading grpcio-1.70.0-cp312-cp312-win_amd64.whl (4.3 MB)\n",
      "   ---------------------------------------- 0.0/4.3 MB ? eta -:--:--\n",
      "   ------------ --------------------------- 1.4/4.3 MB 29.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  4.3/4.3 MB 54.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 4.3/4.3 MB 46.0 MB/s eta 0:00:00\n",
      "Downloading keras-3.8.0-py3-none-any.whl (1.3 MB)\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ---------------------------------------  1.3/1.3 MB 86.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.3/1.3 MB 27.5 MB/s eta 0:00:00\n",
      "Downloading libclang-18.1.1-py2.py3-none-win_amd64.whl (26.4 MB)\n",
      "   ---------------------------------------- 0.0/26.4 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 3.8/26.4 MB 80.8 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 7.1/26.4 MB 75.7 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 9.7/26.4 MB 68.6 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 13.7/26.4 MB 72.6 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 17.5/26.4 MB 81.8 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 19.9/26.4 MB 72.6 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 23.7/26.4 MB 72.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  26.4/26.4 MB 65.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  26.4/26.4 MB 65.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 26.4/26.4 MB 43.5 MB/s eta 0:00:00\n",
      "Downloading ml_dtypes-0.4.1-cp312-cp312-win_amd64.whl (127 kB)\n",
      "   ---------------------------------------- 0.0/127.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 127.5/127.5 kB 7.3 MB/s eta 0:00:00\n",
      "Downloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "   ---------------------------------------- 0.0/71.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 71.9/71.9 kB 3.9 MB/s eta 0:00:00\n",
      "Downloading tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
      "   ---------------------------------------- 0.0/5.5 MB ? eta -:--:--\n",
      "   --------------------------- ------------ 3.8/5.5 MB 80.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  5.5/5.5 MB 70.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  5.5/5.5 MB 58.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  5.5/5.5 MB 58.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.5/5.5 MB 25.1 MB/s eta 0:00:00\n",
      "Downloading termcolor-2.5.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Downloading namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
      "Downloading optree-0.14.0-cp312-cp312-win_amd64.whl (299 kB)\n",
      "   ---------------------------------------- 0.0/299.6 kB ? eta -:--:--\n",
      "   --------------------------------------  297.0/299.6 kB 19.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  297.0/299.6 kB 19.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  297.0/299.6 kB 19.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  297.0/299.6 kB 19.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  297.0/299.6 kB 19.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 299.6/299.6 kB 1.2 MB/s eta 0:00:00\n",
      "Installing collected packages: namex, libclang, flatbuffers, termcolor, tensorboard-data-server, optree, opt-einsum, ml-dtypes, grpcio, google-pasta, gast, astunparse, absl-py, tensorboard, keras, tensorflow-intel, tensorflow\n",
      "Successfully installed absl-py-2.1.0 astunparse-1.6.3 flatbuffers-25.2.10 gast-0.6.0 google-pasta-0.2.0 grpcio-1.70.0 keras-3.8.0 libclang-18.1.1 ml-dtypes-0.4.1 namex-0.0.8 opt-einsum-3.4.0 optree-0.14.0 tensorboard-2.18.0 tensorboard-data-server-0.7.2 tensorflow-2.18.0 tensorflow-intel-2.18.0 termcolor-2.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow tqdm tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5e4d2c7-09c5-4b8a-8834-5f21f2f0fa16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.18.0\n",
      "tqdm version: 4.66.4\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "print(\"TensorFlow version:\", version(\"tensorflow\"))\n",
    "print(\"tqdm version:\", version(\"tqdm\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47b9d7d7-071e-461b-8210-895a7098f792",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import urllib.request\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "\n",
    "def download_file(url, destination, backup_url=None):\n",
    "    def _attempt_download(download_url):\n",
    "        with urllib.request.urlopen(download_url) as response:\n",
    "            # Get the total file size from headers, defaulting to 0 if not present\n",
    "            file_size = int(response.headers.get(\"Content-Length\", 0))\n",
    "\n",
    "            # Check if file exists and has the same size\n",
    "            if os.path.exists(destination):\n",
    "                file_size_local = os.path.getsize(destination)\n",
    "                if file_size == file_size_local:\n",
    "                    print(f\"File already exists and is up-to-date: {destination}\")\n",
    "                    return True  # Indicate success without re-downloading\n",
    "\n",
    "            block_size = 1024  # 1 Kilobyte\n",
    "\n",
    "            # Initialize the progress bar with total file size\n",
    "            progress_bar_description = os.path.basename(download_url)\n",
    "            with tqdm(total=file_size, unit=\"iB\", unit_scale=True, desc=progress_bar_description) as progress_bar:\n",
    "                with open(destination, \"wb\") as file:\n",
    "                    while True:\n",
    "                        chunk = response.read(block_size)\n",
    "                        if not chunk:\n",
    "                            break\n",
    "                        file.write(chunk)\n",
    "                        progress_bar.update(len(chunk))\n",
    "            return True\n",
    "\n",
    "    try:\n",
    "        if _attempt_download(url):\n",
    "            return\n",
    "    except (urllib.error.HTTPError, urllib.error.URLError):\n",
    "        if backup_url is not None:\n",
    "            print(f\"Primary URL ({url}) failed. Attempting backup URL: {backup_url}\")\n",
    "            try:\n",
    "                if _attempt_download(backup_url):\n",
    "                    return\n",
    "            except urllib.error.HTTPError:\n",
    "                pass\n",
    "\n",
    "        # If we reach here, both attempts have failed\n",
    "        error_message = (\n",
    "            f\"Failed to download from both primary URL ({url})\"\n",
    "            f\"{' and backup URL (' + backup_url + ')' if backup_url else ''}.\"\n",
    "            \"\\nCheck your internet connection or the file availability.\\n\"\n",
    "            \"For help, visit: https://github.com/rasbt/LLMs-from-scratch/discussions/273\"\n",
    "        )\n",
    "        print(error_message)\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "def load_gpt2_params_from_tf_ckpt(ckpt_path, settings):\n",
    "    # Initialize parameters dictionary with empty blocks for each layer\n",
    "    params = {\"blocks\": [{} for _ in range(settings[\"n_layer\"])]}\n",
    "\n",
    "    # Iterate over each variable in the checkpoint\n",
    "    for name, _ in tf.train.list_variables(ckpt_path):\n",
    "        # Load the variable and remove singleton dimensions\n",
    "        variable_array = np.squeeze(tf.train.load_variable(ckpt_path, name))\n",
    "\n",
    "        # Process the variable name to extract relevant parts\n",
    "        variable_name_parts = name.split(\"/\")[1:]  # Skip the 'model/' prefix\n",
    "\n",
    "        # Identify the target dictionary for the variable\n",
    "        target_dict = params\n",
    "        if variable_name_parts[0].startswith(\"h\"):\n",
    "            layer_number = int(variable_name_parts[0][1:])\n",
    "            target_dict = params[\"blocks\"][layer_number]\n",
    "\n",
    "        # Recursively access or create nested dictionaries\n",
    "        for key in variable_name_parts[1:-1]:\n",
    "            target_dict = target_dict.setdefault(key, {})\n",
    "\n",
    "        # Assign the variable array to the last key\n",
    "        last_key = variable_name_parts[-1]\n",
    "        target_dict[last_key] = variable_array\n",
    "\n",
    "    return params\n",
    "\n",
    "def download_and_load_gpt2(model_size, models_dir):\n",
    "    # Validate model size\n",
    "    allowed_sizes = (\"124M\", \"355M\", \"774M\", \"1558M\")\n",
    "    if model_size not in allowed_sizes:\n",
    "        raise ValueError(f\"Model size not in {allowed_sizes}\")\n",
    "\n",
    "    # Define paths\n",
    "    model_dir = os.path.join(models_dir, model_size)\n",
    "    base_url = \"https://openaipublic.blob.core.windows.net/gpt-2/models\"\n",
    "    backup_base_url = \"https://f001.backblazeb2.com/file/LLMs-from-scratch/gpt2\"\n",
    "    filenames = [\n",
    "        \"checkpoint\", \"encoder.json\", \"hparams.json\",\n",
    "        \"model.ckpt.data-00000-of-00001\", \"model.ckpt.index\",\n",
    "        \"model.ckpt.meta\", \"vocab.bpe\"\n",
    "    ]\n",
    "\n",
    "    # Download files\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    for filename in filenames:\n",
    "        file_url = os.path.join(base_url, model_size, filename)\n",
    "        backup_url = os.path.join(backup_base_url, model_size, filename)\n",
    "        file_path = os.path.join(model_dir, filename)\n",
    "        download_file(file_url, file_path, backup_url)\n",
    "\n",
    "    # Load settings and params\n",
    "    tf_ckpt_path = tf.train.latest_checkpoint(model_dir)\n",
    "    settings = json.load(open(os.path.join(model_dir, \"hparams.json\")))\n",
    "    params = load_gpt2_params_from_tf_ckpt(tf_ckpt_path, settings)\n",
    "\n",
    "    return settings, params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ea36f04-fae5-476f-a0c9-d1c4e5318a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "checkpoint: 100%|██████████████████████████████████████████████████████████████████| 77.0/77.0 [00:00<00:00, 76.9kiB/s]\n",
      "encoder.json: 100%|███████████████████████████████████████████████████████████████| 1.04M/1.04M [00:01<00:00, 842kiB/s]\n",
      "hparams.json: 100%|████████████████████████████████████████████████████████████████| 90.0/90.0 [00:00<00:00, 43.5kiB/s]\n",
      "model.ckpt.data-00000-of-00001: 100%|██████████████████████████████████████████████| 498M/498M [01:14<00:00, 6.67MiB/s]\n",
      "model.ckpt.index: 100%|██████████████████████████████████████████████████████████| 5.21k/5.21k [00:00<00:00, 2.61MiB/s]\n",
      "model.ckpt.meta: 100%|██████████████████████████████████████████████████████████████| 471k/471k [00:00<00:00, 536kiB/s]\n",
      "vocab.bpe: 100%|████████████████████████████████████████████████████████████████████| 456k/456k [00:00<00:00, 623kiB/s]\n"
     ]
    }
   ],
   "source": [
    "settings, params = download_and_load_gpt2(model_size=\"124M\", models_dir=\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a0f2e5fc-aed6-43fd-9e5a-8587a410ffd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Importação das bibliotecas\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5c01bcab-552b-492f-beb7-277f21acaba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Configuração do dispositivo\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9a173732-8404-402c-8712-8a5be13b9c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Configuração do modelo GPT-2 otimizada\n",
    "GPT_CONFIG = {\n",
    "    \"vocab_size\": 50257,  # Tamanho do vocabulário\n",
    "    \"context_length\": 256,  # Comprimento máximo do contexto\n",
    "    \"emb_dim\": 768,  # Dimensão do embedding\n",
    "    \"n_heads\": 12,  # Número de cabeças de atenção\n",
    "    \"n_layers\": 12,  # Número de camadas\n",
    "    \"drop_rate\": 0.1,  # Taxa de dropout\n",
    "    \"qkv_bias\": True  # Uso de bias nas projeções de atenção\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7d63b6a6-7279-43f2-a1f1-7df715742e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Inicialização do tokenizer\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "def encode_text(text):\n",
    "    return torch.tensor(tokenizer.encode(text), dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "def decode_text(tokens):\n",
    "    return tokenizer.decode(tokens.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "04afcb8d-3d2b-4d7e-a9ae-0f34b7d39f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Construção do modelo GPT\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(config[\"vocab_size\"], config[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(config[\"context_length\"], config[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(config[\"drop_rate\"])\n",
    "        self.trf_blocks = nn.Sequential(*[TransformerBlock(config) for _ in range(config[\"n_layers\"])] )\n",
    "        self.final_norm = LayerNorm(config[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(config[\"emb_dim\"], config[\"vocab_size\"], bias=False)\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        return self.out_head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2d83b9ac-c30b-4bdc-b442-0a76dcf4bc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Construção do modelo GPT\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(config[\"vocab_size\"], config[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(config[\"context_length\"], config[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(config[\"drop_rate\"])\n",
    "        self.trf_blocks = nn.Sequential(*[TransformerBlock(config) for _ in range(config[\"n_layers\"])] )\n",
    "        self.final_norm = LayerNorm(config[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(config[\"emb_dim\"], config[\"vocab_size\"], bias=False)\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        return self.out_head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "fcaca4f4-1b68-4524-b632-12d0dd7cf8a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Ouviram do Ipiranga as margens plácidasO%01>*HI'MQD\"+N(-IKM\n"
     ]
    }
   ],
   "source": [
    "# 6. Geração de texto aprimorada\n",
    "def generate_text(model, tokenizer, prompt, max_new_tokens=20, temperature=1.0, top_k=50):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    input_tokens = encode_text(prompt)\n",
    "    generated_tokens = input_tokens.tolist()[0]\n",
    "    \n",
    "    for _ in range(max_new_tokens):\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_tokens)[:, -1, :] / temperature  # Ajuste de temperatura\n",
    "            logits = torch.topk(logits, top_k)[0]  # Aplica top-k sampling\n",
    "            probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1).item()\n",
    "            generated_tokens.append(next_token)\n",
    "            input_tokens = torch.tensor([generated_tokens], dtype=torch.long).to(device)\n",
    "    \n",
    "    output_text = decode_text(torch.tensor(generated_tokens))\n",
    "    print(\"Output text:\\n\", output_text)\n",
    "\n",
    "# Exemplo de Entrada: Hino Nacional Brasileiro\n",
    "prompt = \"Ouviram do Ipiranga as margens plácidas\"\n",
    "generate_text(GPTModel(GPT_CONFIG).to(device), tokenizer, prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0061bacf-06dc-4291-afc8-f2a11d69e5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
