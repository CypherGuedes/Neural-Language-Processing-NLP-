{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "b62c35d6-015c-43e5-8931-239933209e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicação completa:\n",
    "# Esta aplicação faz uso do tokenizador BPE (Byte Pair Encoding) para lidar com textos e realizar a tokenização de forma eficiente, especialmente útil para sistemas NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "5909099d-9d1a-4ffc-88e1-01ea4945a8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa as bibliotecas necessárias para a aplicação:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "4e3b097d-406c-40e3-8229-60d04c18b207",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  # Para manipulação de diretórios e arquivos.\n",
    "import json  # Para carregar e manipular dados em formato JSON.\n",
    "from tqdm import tqdm  # Exibe uma barra de progresso durante operações demoradas.\n",
    "import sys  # Permite manipular parâmetros e caminhos do sistema.\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "7ada9bfe-d3ac-46f2-b6c7-6eed233a22e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define o caminho para a pasta 'corpus', que deve conter os arquivos JSON a serem processados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "f083fac2-e588-4172-83d3-e250fe61f48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pasta_corpus = r'C:\\Users\\tagsa\\Downloads\\corpus'  # O 'r' antes da string indica uma raw string, preservando as barras invertidas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "8bbc328f-f08d-49bd-b167-605f0943e0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carrega e combina todos os textos dos arquivos JSON em uma única string.\n",
    "# Exibe no console os resultados do processamento, fornecendo informações sobre o corpus carregado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "24a868b0-7dde-41f7-b8f1-dca323123844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de arquivos JSON encontrados: 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Carregando arquivos JSON: 100%|████████████████████████████████████████████| 10000/10000 [00:15<00:00, 629.30arquivo/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de arquivos JSON válidos lidos: 10000\n",
      "Corpus carregado com 71023261 caracteres.\n",
      "Número de textos individuais: 10000\n"
     ]
    }
   ],
   "source": [
    "corpus_texto = carregar_textos_json(pasta_corpus)\n",
    "\n",
    "print(f\"Número de arquivos JSON válidos lidos: {arquivos_validos}\")  # Exibe o número de arquivos JSON válidos lidos.\n",
    "print(f\"Corpus carregado com {sum(len(texto) for texto in textos)} caracteres.\")  # Exibe o tamanho do corpus, ou seja, a quantidade total de caracteres nos textos carregados.\n",
    "print(f\"Número de textos individuais: {textos_encontrados}\")  # Exibe o número total de textos individuais extraídos dos arquivos JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "5a7cae07-7c40-4f1f-9e76-898f9a006067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adiciona ao sys.path o diretório onde o módulo BPE_Tokenizer pode estar localizado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "c947879b-65ff-497e-bb4e-efa5cbb83cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(r\"C:\\Users\\tagsa\\Downloads\")  # Use raw string para evitar erros com barras invertidas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "29bb661c-a290-4e09-a9cb-f542e7e0e4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa o tokenizador BPE do módulo localizado no caminho adicionado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "b187ecd4-5ce1-4e5d-84a0-5438bacb0b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bpe_tokenizer_otimizado_e_comentado import BPE_Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "b22811f7-643c-44b7-91a5-bd256bb29573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializa o tokenizador BPE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "05a4f230-88f5-4e2a-a5da-6cba210f769e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BPE_Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "4618c473-ac60-4e46-9f4c-8e75cf900d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta função realiza a leitura de arquivos JSON armazenados em uma pasta específica.\n",
    "# O objetivo é extrair textos desses arquivos e calcular quantos textos e arquivos foram processados com sucesso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "2e4a6584-0cd7-40fe-ae4f-6328ae041983",
   "metadata": {},
   "outputs": [],
   "source": [
    "def carregar_textos_json(pasta):\n",
    "    \"\"\"\n",
    "    Carrega e combina todos os textos dos arquivos JSON em um único texto.\n",
    "\n",
    "    Parâmetros:\n",
    "    -----------\n",
    "    pasta : str\n",
    "        Caminho da pasta onde estão localizados os arquivos JSON.\n",
    "\n",
    "    Retorna:\n",
    "    --------\n",
    "    str:\n",
    "        Todo o conteúdo dos arquivos JSON combinados em uma única string.\n",
    "    \"\"\"\n",
    "    corpus = []\n",
    "\n",
    "    if not os.path.exists(pasta):\n",
    "        print(f\"Pasta não encontrada: {pasta}\")\n",
    "        return \"\"\n",
    "\n",
    "    arquivos = [arq for arq in os.listdir(pasta) if arq.endswith('.json')]\n",
    "    print(f\"Número de arquivos JSON encontrados: {len(arquivos)}\")\n",
    "\n",
    "    for arquivo in tqdm(arquivos, desc=\"Carregando arquivos JSON\", unit=\"arquivo\"):\n",
    "        caminho = os.path.join(pasta, arquivo)\n",
    "        try:\n",
    "            with open(caminho, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "            if 'text' in data:\n",
    "                corpus.append(data['text'])\n",
    "        except (json.JSONDecodeError, OSError) as e:\n",
    "            print(f\"Erro ao processar {arquivo}: {str(e)}\")\n",
    "\n",
    "    return \" \".join(corpus)  # Combina todos os textos em uma única string.\n",
    "\n",
    "def get_most_frequent_pairs(ids):\n",
    "    \"\"\"\n",
    "    Calcula a frequência de pares consecutivos de tokens na lista de IDs.\n",
    "    \"\"\"\n",
    "    pairs = defaultdict(int)\n",
    "    for i in range(len(ids) - 1):\n",
    "        pair = (ids[i], ids[i + 1])\n",
    "        pairs[pair] += 1\n",
    "    return pairs\n",
    "\n",
    "def mesclar_pares(ids, tokenizer, merges, limite=20):\n",
    "    \"\"\"\n",
    "    Realiza a mesclagem de no máximo 20 pares de tokens mais frequentes.\n",
    "    \"\"\"\n",
    "    stats = tokenizer.get_stats(ids)\n",
    "\n",
    "    # Ordena os pares pela frequência em ordem decrescente.\n",
    "    pares_ordenados = sorted(stats.items(), key=lambda x: x[1], reverse=True)[:limite]\n",
    "\n",
    "    for idx, (pair, freq) in enumerate(pares_ordenados):\n",
    "        token_a = tokenizer.vocab.get(pair[0], b'?').decode('utf-8', errors='replace')\n",
    "        token_b = tokenizer.vocab.get(pair[1], b'?').decode('utf-8', errors='replace')\n",
    "\n",
    "        print(f\"Mesclando {pair} ({token_a}, {token_b}) em um novo token {256 + idx} com frequência {freq}\")\n",
    "\n",
    "        ids = tokenizer.merge(ids, pair, 256 + idx)\n",
    "        merges[pair] = (256 + idx, freq)\n",
    "\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "d49de72a-e223-4801-85b7-cc53a75e0951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinamento do tokenizador BPE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "dca9f97a-d5af-4edc-a9de-aec30317d90e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treinando o tokenizador BPE...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinamento do tokenizador: 100%|████████████████████████████████████████████████| 20/20 [07:08<00:00, 21.41s/iteração]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treinamento do tokenizador BPE concluído!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Treinando o tokenizador BPE...\")\n",
    "tokenizer.train(corpus_texto, vocab_size=276)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "7a20672e-d9ac-4ea2-ab86-f018e10db837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializa o dicionário de merges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "8e955496-936a-4766-98b0-056b595a2992",
   "metadata": {},
   "outputs": [],
   "source": [
    "merges = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "297827d6-baf2-4a96-8d76-b18a45a8d078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converte o corpus para uma lista de IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "67af5ac8-5598-4b85-b4b3-8f85507e96b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = list(corpus_texto.encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "8ee076d8-b04f-4a89-bac3-93da8e352872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realiza a mesclagem dos 20 pares mais frequentes (conforme definido)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "a28da01d-840c-46dc-969c-f559756cc2cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mesclando (111, 32) (o,  ) em um novo token 256 com frequência 1934443\n",
      "Mesclando (97, 32) (a,  ) em um novo token 257 com frequência 1767265\n",
      "Mesclando (101, 32) (e,  ) em um novo token 258 com frequência 1690522\n",
      "Mesclando (115, 32) (s,  ) em um novo token 259 com frequência 1397720\n",
      "Mesclando (32, 100) ( , d) em um novo token 260 com frequência 1372702\n",
      "Mesclando (100, 101) (d, e) em um novo token 261 com frequência 1069531\n",
      "Mesclando (32, 101) ( , e) em um novo token 262 com frequência 831530\n",
      "Mesclando (44, 32) (,,  ) em um novo token 263 com frequência 799119\n",
      "Mesclando (114, 97) (r, a) em um novo token 264 com frequência 768678\n",
      "Mesclando (101, 115) (e, s) em um novo token 265 com frequência 768076\n",
      "Mesclando (32, 97) ( , a) em um novo token 266 com frequência 765466\n",
      "Mesclando (100, 111) (d, o) em um novo token 267 com frequência 736397\n",
      "Mesclando (111, 115) (o, s) em um novo token 268 com frequência 683013\n",
      "Mesclando (32, 112) ( , p) em um novo token 269 com frequência 658197\n",
      "Mesclando (97, 115) (a, s) em um novo token 270 com frequência 623501\n",
      "Mesclando (101, 110) (e, n) em um novo token 271 com frequência 619757\n",
      "Mesclando (110, 116) (n, t) em um novo token 272 com frequência 612902\n",
      "Mesclando (109, 32) (m,  ) em um novo token 273 com frequência 611796\n",
      "Mesclando (32, 99) ( , c) em um novo token 274 com frequência 608601\n",
      "Mesclando (111, 114) (o, r) em um novo token 275 com frequência 603283\n"
     ]
    }
   ],
   "source": [
    "ids = mesclar_pares(ids, tokenizer, merges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "84f23ae5-e6fd-42f1-9745-b3a42ec55d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exibe estatísticas finais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "d39dcf20-b476-405f-8261-be9464d61cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comprimento final dos tokens: 71033260\n",
      "Comprimento final dos IDs: 60952296\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nComprimento final dos tokens: {len(corpus_texto)}\")\n",
    "print(f\"Comprimento final dos IDs: {len(ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "c90277b5-b5cf-4486-9794-0d7e1f95144d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcula e exibe a taxa de compressão."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "b3dc7063-78a2-4f88-970b-caf2c402225c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taxa de compressão: 1.17X\n"
     ]
    }
   ],
   "source": [
    "taxa_compressao = len(corpus_texto) / len(ids)\n",
    "print(f\"Taxa de compressão: {taxa_compressao:.2f}X\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
