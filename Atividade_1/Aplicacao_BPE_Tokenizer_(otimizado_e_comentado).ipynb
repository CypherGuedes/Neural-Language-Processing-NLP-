{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b62c35d6-015c-43e5-8931-239933209e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicação completa: Esta aplicação faz uso do tokenizador BPE (Byte Pair Encoding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5909099d-9d1a-4ffc-88e1-01ea4945a8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa as bibliotecas necessárias para a aplicação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e3b097d-406c-40e3-8229-60d04c18b207",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  # Para manipulação de diretórios e arquivos.\n",
    "import json  # Para carregar e manipular dados em formato JSON.\n",
    "from tqdm import tqdm  # Exibe uma barra de progresso durante operações demoradas.\n",
    "import sys  # Permite manipular parâmetros e caminhos do sistema.\n",
    "from collections import defaultdict  # Cria dicionários com valor padrão automático.\n",
    "\n",
    "sys.path.append(r\"C:\\Users\\tagsa\\Downloads\")  # Adiciona ao sys.path o diretório onde o módulo BPE_Tokenizer pode estar localizado.\n",
    "from bpe_tokenizer_otimizado_e_comentado import BPE_Tokenizer  # Importa o tokenizador BPE do módulo localizado no caminho adicionado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b22811f7-643c-44b7-91a5-bd256bb29573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializa o tokenizador BPE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05a4f230-88f5-4e2a-a5da-6cba210f769e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BPE_Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ada9bfe-d3ac-46f2-b6c7-6eed233a22e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define o caminho para a pasta 'corpus', que deve conter os arquivos a serem processados (esta aplicaçao são arquivos JSON)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f083fac2-e588-4172-83d3-e250fe61f48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pasta_corpus = r'C:\\Users\\tagsa\\Downloads\\corpus'  # O 'r' antes da string indica uma raw string, preservando as barras invertidas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e4a6584-0cd7-40fe-ae4f-6328ae041983",
   "metadata": {},
   "outputs": [],
   "source": [
    "def carregar_textos_json(pasta):\n",
    "    \"\"\"\n",
    "    Carrega e combina todos os textos dos arquivos JSON em uma única string.\n",
    "\n",
    "    Parâmetros:\n",
    "    pasta : str\n",
    "        Caminho para a pasta onde os arquivos JSON estão localizados.\n",
    "\n",
    "    Retorna:\n",
    "    tuple\n",
    "        - String com o corpus combinado.\n",
    "        - Número de arquivos JSON válidos lidos.\n",
    "        - Contador de textos individuais encontrados.\n",
    "    \"\"\"\n",
    "    corpus = []  # Lista para armazenar o conteúdo dos arquivos.\n",
    "    arquivos_validos = 0  # Contador de arquivos JSON válidos.\n",
    "    textos_encontrados = 0  # Contador de textos individuais.\n",
    "\n",
    "    if not os.path.exists(pasta):\n",
    "        print(f\"Pasta não encontrada: {pasta}\")\n",
    "        return \"\", 0, 0  # Retorna valores padrão no caso de erro.\n",
    "\n",
    "    # Lista os arquivos JSON encontrados na pasta.\n",
    "    arquivos = [arq for arq in os.listdir(pasta) if arq.endswith(\".json\")]\n",
    "    print(f\"Número de arquivos JSON encontrados: {len(arquivos)}\")\n",
    "\n",
    "    # Itera sobre os arquivos e processa cada um.\n",
    "    for arquivo in tqdm(arquivos, desc=\"Carregando arquivos JSON\", unit=\"arquivo\"):\n",
    "        caminho = os.path.join(pasta, arquivo)\n",
    "        try:\n",
    "            with open(caminho, 'r', encoding='utf-8') as f:\n",
    "                data = f.read()  # Lê o conteúdo do arquivo.\n",
    "                corpus.append(data)  # Adiciona o conteúdo ao corpus.\n",
    "                arquivos_validos += 1  # Incrementa o contador de arquivos válidos.\n",
    "                textos_encontrados += len(data.split())  # Conta textos individuais.\n",
    "        except (json.JSONDecodeError, OSError) as erro:\n",
    "            print(f\"Erro ao processar {arquivo}: {erro}\")\n",
    "\n",
    "    return \" \".join(corpus), arquivos_validos, textos_encontrados  # Retorna a tupla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "993bf99c-1a5c-477c-8c59-dd49a807fe64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contar a frequência de pares de tokens consecutivos em uma lista de IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1ba8fed5-7980-4321-8483-c652764b3a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_frequent_pairs(ids):\n",
    "    \"\"\"\n",
    "    Calcula a frequência de pares consecutivos de tokens na lista de IDs.\n",
    "\n",
    "    Parâmetros:\n",
    "    ids : list of list of int\n",
    "        Lista contendo sequências de IDs de tokens.\n",
    "\n",
    "    Retorna:\n",
    "    dict\n",
    "        Dicionário com pares de tokens como chave e suas frequências como valor.\n",
    "    \"\"\"\n",
    "    pairs = defaultdict(int)  # Inicializa um dicionário com valor padrão 0 para cada par.\n",
    "\n",
    "    # Itera sobre cada sequência de IDs na lista.\n",
    "    for id_seq in ids:\n",
    "        # Itera sobre os índices da sequência para formar pares consecutivos.\n",
    "        for i in range(len(id_seq) - 1):\n",
    "            pair = (id_seq[i], id_seq[i + 1])  # Cria um par com dois tokens consecutivos.\n",
    "            pairs[pair] += 1  # Incrementa a contagem do par no dicionário.\n",
    "\n",
    "    return pairs  # Retorna o dicionário de pares e suas frequências."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d94fce18-2cb6-4f13-a538-10ff96d3ddce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para realizar a mesclagem dos pares mais frequentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2fe89e05-7364-4200-8e25-ac2f63c66f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizar a mesclagem dos pares de tokens mais frequentes até um máximo de 20.\n",
    "def merge_pairs(ids, tokenizar, limite=20):\n",
    "    \"\"\"\n",
    "    Realiza a mesclagem dos pares mais frequentes com base no limite fornecido.\n",
    "\n",
    "    Parâmetros:\n",
    "    ids : list of list of int\n",
    "        Lista de IDs a serem processados.\n",
    "    tokenizar : objeto\n",
    "        Objeto que contém métodos de tokenização.\n",
    "    limite : int, opcional\n",
    "        Limite de pares a serem mesclados (padrão: 20).\n",
    "\n",
    "    Retorna:\n",
    "    list of list of int\n",
    "        Lista de IDs atualizada após as mesclagens.\n",
    "    \"\"\"\n",
    "    # Obtém estatísticas de pares.\n",
    "    status = tokenizar.get_stats(ids)\n",
    "    pares_ordenados = sorted(status.items(), key=lambda x: x[1], reverse=True)[:limite]\n",
    "\n",
    "    global merge_count  # Usa o contador global para acompanhar os novos tokens.\n",
    "\n",
    "    for (a, b), freq in pares_ordenados:\n",
    "        # Obtém os caracteres correspondentes aos IDs.\n",
    "        token_a = tokenizar.vocab.get(a, b'\\x7f').decode('utf-8', errors='replace')\n",
    "        token_b = tokenizar.vocab.get(b, b'\\x7f').decode('utf-8', errors='replace')\n",
    "\n",
    "        # Exibe no formato solicitado.\n",
    "        print(f\"Mesclando ({a}, {b}) ({token_a}, {token_b}) em um novo token {merge_count} com frequência {freq}\")\n",
    "\n",
    "        # Realiza a mesclagem e incrementa o contador de tokens.\n",
    "        ids = tokenizar.merge(ids, (a, b), merge_count)\n",
    "        merge_count += 1  # Incrementa o contador de tokens.\n",
    "\n",
    "    return ids  # Retorna a lista de IDs atualizada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8bbc328f-f08d-49bd-b167-605f0943e0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carrega e combina todos os textos dos arquivos JSON em uma única string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "994af1fa-a729-4c63-8f20-6e79e34a6b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de arquivos JSON encontrados: 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Carregando arquivos JSON: 100%|███████████████████████████████████████████| 10000/10000 [00:02<00:00, 4293.61arquivo/s]\n"
     ]
    }
   ],
   "source": [
    "corpus_texto, arquivos_validos, textos_encontrados = carregar_textos_json(pasta_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "708dd8b0-ccd8-4c67-ad9b-144f34ab81ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exibe no console os resultados do processamento, fornecendo informações sobre o corpus carregado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "24a868b0-7dde-41f7-b8f1-dca323123844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de arquivos JSON válidos lidos: 10000\n",
      "Corpus carregado com 71738184 caracteres.\n",
      "Número de textos individuais: 11555410\n"
     ]
    }
   ],
   "source": [
    "print(f\"Número de arquivos JSON válidos lidos: {arquivos_validos}\")  # Exibe as informações sobre o corpus carregado.\n",
    "print(f\"Corpus carregado com {len(corpus_texto)} caracteres.\")  # Exibe o total de caracteres no corpus carregado.\n",
    "print(f\"Número de textos individuais: {textos_encontrados}\")  # Exibe o número total de textos individuais encontrados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d49de72a-e223-4801-85b7-cc53a75e0951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinamento do tokenizador BPE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dca9f97a-d5af-4edc-a9de-aec30317d90e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Treinamento do tokenizador: 100%|████████████████████████████████████████████████| 20/20 [08:16<00:00, 24.85s/iteração]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treinamento do tokenizador BPE concluído!\n"
     ]
    }
   ],
   "source": [
    "tokenizer.train(corpus_texto, vocab_size=276)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "48fbdac1-6660-4130-8fc2-147ba9cd82a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de mesclagem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7a20672e-d9ac-4ea2-ab86-f018e10db837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mesclando (111, 32) (o,  ) em um novo token 256 com frequência 1936188\n",
      "Mesclando (97, 32) (a,  ) em um novo token 257 com frequência 1768162\n",
      "Mesclando (101, 32) (e,  ) em um novo token 258 com frequência 1692138\n",
      "Mesclando (115, 32) (s,  ) em um novo token 259 com frequência 1397165\n",
      "Mesclando (32, 100) ( , d) em um novo token 260 com frequência 1374945\n",
      "Mesclando (100, 101) (d, e) em um novo token 261 com frequência 1071607\n",
      "Mesclando (32, 101) ( , e) em um novo token 262 com frequência 831685\n",
      "Mesclando (44, 32) (,,  ) em um novo token 263 com frequência 799147\n",
      "Mesclando (114, 97) (r, a) em um novo token 264 com frequência 770498\n",
      "Mesclando (101, 115) (e, s) em um novo token 265 com frequência 769312\n",
      "Mesclando (32, 97) ( , a) em um novo token 266 com frequência 765729\n",
      "Mesclando (100, 111) (d, o) em um novo token 267 com frequência 737449\n",
      "Mesclando (111, 115) (o, s) em um novo token 268 com frequência 683818\n",
      "Mesclando (32, 112) ( , p) em um novo token 269 com frequência 658313\n",
      "Mesclando (97, 115) (a, s) em um novo token 270 com frequência 624507\n",
      "Mesclando (101, 110) (e, n) em um novo token 271 com frequência 620654\n",
      "Mesclando (110, 116) (n, t) em um novo token 272 com frequência 613959\n",
      "Mesclando (116, 101) (t, e) em um novo token 273 com frequência 611882\n",
      "Mesclando (109, 32) (m,  ) em um novo token 274 com frequência 611864\n",
      "Mesclando (32, 99) ( , c) em um novo token 275 com frequência 608702\n"
     ]
    }
   ],
   "source": [
    "merge_count = 256  # Inicializa um contador para os novos tokens.\n",
    "ids = list(corpus_texto.encode(\"utf-8\"))  # Converte o corpus para uma lista de IDs.\n",
    "ids = merge_pairs(ids, tokenizer)  # Realiza a mesclagem dos 20 pares mais frequentes (conforme definido)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "84f23ae5-e6fd-42f1-9745-b3a42ec55d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exibe estatísticas finais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d39dcf20-b476-405f-8261-be9464d61cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comprimento final dos tokens: 71738184\n",
      "Comprimento final dos IDs: 61881386\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\nComprimento final dos tokens: {len(corpus_texto)}\")  # Calcula o número total de caracteres presentes no corpus original após o processo de tokenização.\n",
    "print(f\"Comprimento final dos IDs: {len(ids)}\")  # Exibe a quantidade de IDs (tokens únicos) gerados após a aplicação do tokenizador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c90277b5-b5cf-4486-9794-0d7e1f95144d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcula e exibe a taxa de compressão."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b3dc7063-78a2-4f88-970b-caf2c402225c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taxa de compressão: 1.16X\n"
     ]
    }
   ],
   "source": [
    "taxa_compressao = len(corpus_texto) / len(ids)  # Divide o comprimento do texto original pelo número de IDs para indicar a eficiência da tokenização.\n",
    "print(f\"Taxa de compressão: {taxa_compressao:.2f}X\")  # Formata a saída para exibir a taxa com duas casas decimais"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
