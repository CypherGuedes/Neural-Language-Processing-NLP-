{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b62c35d6-015c-43e5-8931-239933209e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicação completa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "4e3b097d-406c-40e3-8229-60d04c18b207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de arquivos JSON encontrados: 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Carregando arquivos JSON: 100%|████████████████████████████████████████████| 10000/10000 [00:16<00:00, 600.79arquivo/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de arquivos JSON válidos lidos: 10000\n",
      "Corpus carregado com 71033256 caracteres.\n",
      "Número de textos individuais: 49977754\n",
      "Treinando o tokenizador BPE...\n",
      "Iteração 0/20 em andamento.\n",
      "Iteração 10/20 em andamento.\n",
      "Treinamento do tokenizador BPE concluído!\n",
      "Listando tokens...\n",
      "Mesclando (195, 163) (�, �) em um novo token 256 com frequência 378704\n",
      "Mesclando (97, 100) (a, d) em um novo token 257 com frequência 325641\n",
      "Mesclando (262, 116) (en, t) em um novo token 258 com frequência 322247\n",
      "Mesclando (256, 256) (o , o ) em um novo token 259 com frequência 320272\n",
      "Mesclando (195, 167) (�, �) em um novo token 260 com frequência 309541\n",
      "Mesclando (114, 105) (r, i) em um novo token 261 com frequência 308050\n",
      "Mesclando (99, 105) (c, i) em um novo token 262 com frequência 304864\n",
      "Mesclando (97, 260) (a, , ) em um novo token 263 com frequência 294469\n",
      "Mesclando (114, 101) (r, e) em um novo token 264 com frequência 294374\n",
      "Mesclando (113, 117) (q, u) em um novo token 265 com frequência 290190\n",
      "Mesclando (115, 116) (s, t) em um novo token 266 com frequência 286657\n",
      "Mesclando (100, 257) (d, a ) em um novo token 267 com frequência 285359\n",
      "Mesclando (97, 116) (a, t) em um novo token 268 com frequência 262392\n",
      "Mesclando (195, 169) (�, �) em um novo token 269 com frequência 257792\n",
      "Mesclando (111, 110) (o, n) em um novo token 270 com frequência 246605\n",
      "Mesclando (101, 108) (e, l) em um novo token 271 com frequência 246232\n",
      "Mesclando (101, 259) (e, s ) em um novo token 272 com frequência 245977\n",
      "Mesclando (101, 263) (e, m ) em um novo token 273 com frequência 232590\n",
      "Mesclando (105, 99) (i, c) em um novo token 274 com frequência 232507\n",
      "Mesclando (97, 115) (a, s) em um novo token 275 com frequência 207546\n",
      "\n",
      "Comprimento final dos tokens: 57540923\n",
      "Comprimento final dos IDs: 51890330\n",
      "Taxa de compressão: 1.11X\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define o caminho para a pasta 'corpus'\n",
    "pasta_corpus = r'C:\\Users\\tagsa\\Downloads\\corpus'\n",
    "\n",
    "# Função para carregar e verificar os textos dos arquivos JSON\n",
    "def carregar_textos_json(pasta):\n",
    "    textos = []\n",
    "    arquivos_validos = 0\n",
    "    textos_encontrados = 0\n",
    "\n",
    "    if not os.path.exists(pasta):\n",
    "        print(f\"Pasta não encontrada: {pasta}\")\n",
    "        return '', [], 0, 0\n",
    "\n",
    "    arquivos = [arq for arq in os.listdir(pasta) if arq.endswith('.json')]\n",
    "    print(f\"Número de arquivos JSON encontrados: {len(arquivos)}\")\n",
    "\n",
    "    for arquivo in tqdm(arquivos, desc=\"Carregando arquivos JSON\", unit=\"arquivo\"):\n",
    "        caminho = os.path.join(pasta, arquivo)\n",
    "        try:\n",
    "            with open(caminho, \"r\", encoding=\"utf-8\") as f:\n",
    "                dados = json.load(f)\n",
    "                arquivos_validos += 1\n",
    "\n",
    "                if isinstance(dados, list):\n",
    "                    textos.extend(item.get('text', '') for item in dados if item.get('text'))\n",
    "                elif isinstance(dados, dict):\n",
    "                    texto = dados.get('text', '')\n",
    "                    if texto:\n",
    "                        textos.append(texto)\n",
    "\n",
    "                textos_encontrados += len(textos)\n",
    "\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Erro ao ler o arquivo: {arquivo}\")\n",
    "\n",
    "    corpus = ' '.join(textos)\n",
    "    return corpus, textos, arquivos_validos, textos_encontrados\n",
    "\n",
    "# Carrega o corpus e os textos\n",
    "corpus, textos, arquivos_validos, textos_encontrados = carregar_textos_json(pasta_corpus)\n",
    "\n",
    "# Exibe resultados\n",
    "print(f\"Número de arquivos JSON válidos lidos: {arquivos_validos}\")\n",
    "print(f\"Corpus carregado com {len(corpus)} caracteres.\")\n",
    "print(f\"Número de textos individuais: {textos_encontrados}\")\n",
    "\n",
    "# Adiciona o caminho do diretório onde está o bpe_tokenizer.py\n",
    "sys.path.append(r'C:\\Users\\tagsa\\Downloads')\n",
    "print(\"Treinando o tokenizador BPE...\")\n",
    "\n",
    "# Define o tamanho do vocabulário e o número de merges\n",
    "vocab_size = 276\n",
    "num_merges = vocab_size - 256\n",
    "\n",
    "# Instancia o tokenizador\n",
    "tokenizer = BPE_Tokenizer()\n",
    "tokenizer.train(corpus, vocab_size)\n",
    "\n",
    "# Lista inicial de tokens codificados\n",
    "print(\"Listando tokens...\")\n",
    "tokens = tokenizer.encode(corpus)\n",
    "ids = list(tokens)  # Converte para lista de IDs\n",
    "\n",
    "# Dicionário para armazenar os merges realizados\n",
    "merges = {}\n",
    "\n",
    "# Função auxiliar para decodificar um par de tokens\n",
    "def decodificar_par(pair, tokenizer):\n",
    "    # Decodifica cada token individualmente\n",
    "    token_a = tokenizer.decode([pair[0]])\n",
    "    token_b = tokenizer.decode([pair[1]])\n",
    "    return token_a, token_b\n",
    "\n",
    "# Realiza os merges e exibe a frequência de cada novo token criado\n",
    "for i in range(num_merges):\n",
    "    # Obtém as estatísticas dos pares mais frequentes\n",
    "    stats = tokenizer.get_stats(ids)\n",
    "    pair = max(stats, key=stats.get)  # Par mais frequente\n",
    "    freq = stats[pair]  # Frequência do par\n",
    "    idx = 256 + i  # Índice do novo token\n",
    "\n",
    "    # Decodifica os tokens do par para exibir seus caracteres\n",
    "    token_a, token_b = decodificar_par(pair, tokenizer)\n",
    "\n",
    "    # Exibe o par sendo mesclado, o novo token, sua frequência e os caracteres\n",
    "    print(f\"Mesclando {pair} ({token_a}, {token_b}) em um novo token {idx} com frequência {freq}\")\n",
    "\n",
    "    # Realiza o merge e armazena no dicionário de merges\n",
    "    ids = tokenizer.merge(ids, pair, idx)\n",
    "    merges[pair] = (idx, freq)  # Armazena o índice e a frequência\n",
    "\n",
    "# Exibe o comprimento dos tokens e IDs após o processo\n",
    "print(\"\\nComprimento final dos tokens:\", len(tokens))\n",
    "print(\"Comprimento final dos IDs:\", len(ids))\n",
    "\n",
    "# Calcula e exibe a taxa de compressão\n",
    "taxa_compressao = len(tokens) / len(ids)\n",
    "print(f\"Taxa de compressão: {taxa_compressao:.2f}X\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
