# -*- coding: utf-8 -*-
"""Cópia de Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_scvZ4NPvOi_APK-DPUozOb6S0mUnE-l
"""

import tiktoken
import torch
import math
import random
from collections import defaultdict
from nltk.tokenize import sent_tokenize

class BigramModel:
    """
    Classe para o modelo de bigrama que utiliza um tokenizador BPE para segmentação e geração de texto.
    """

    def __init__(self, tokenizer):
        """
        Inicializa o modelo de bigrama.
        :param tokenizer: Instância do tokenizador BPE que será utilizado para segmentar o texto.
        """
        self.tokenizer = tokenizer
        self.bigram_counts = defaultdict(lambda: defaultdict(int))
        self.prob_matrix = {}

    def prepare_data(self, text):
        """
        Prepara os dados para o treinamento, adicionando tokens especiais e dividindo em treino e teste.
        :param text: String de entrada contendo o texto completo.
        :return: Tupla contendo listas de tokens para treino e teste.
        """
        sentences = sent_tokenize(text)  # Segmenta o texto em sentenças
        tokens = []

        # Adiciona tokens especiais para cada sentença
        for sentence in sentences:
            encoded_sentence = self.tokenizer.encode(
                f"<|startoftext|> {sentence} <|endoftext|>",
                allowed_special={'<|startoftext|>', '<|endoftext|>'}
            )
            tokens.extend(encoded_sentence)

        # Embaralha e divide os dados em 80% treino e 20% teste
        random.shuffle(tokens)
        train_size = int(0.8 * len(tokens))
        train_tokens = tokens[:train_size]
        test_tokens = tokens[train_size:]

        return train_tokens, test_tokens

    def train(self, train_tokens):
        """
        Treina o modelo de bigrama, contando a frequência dos pares de tokens consecutivos (bigramas).
        Aplica suavização de Laplace para evitar probabilidades zero.
        :param train_tokens: Lista de tokens do conjunto de treino.
        """
        vocab_size = len(set(train_tokens))  # Tamanho do vocabulário para suavização

        for i in range(len(train_tokens) - 1):
            w1 = train_tokens[i]
            w2 = train_tokens[i + 1]
            self.bigram_counts[w1][w2] += 1

        # Calcula as probabilidades condicionais com suavização de Laplace
        for w1, next_words in self.bigram_counts.items():
            total_count = sum(next_words.values()) + vocab_size  # Adiciona vocab_size para suavização
            self.prob_matrix[w1] = {w2: (count + 1) / total_count for w2, count in next_words.items()}  # +1 para suavização
            # Adiciona probabilidade para palavras que não apareceram após w1
            for w2 in range(vocab_size):
                if w2 not in next_words:
                    self.prob_matrix[w1][w2] = 1 / total_count

    def calculate_perplexity(self, test_tokens):
        """
        Calcula a perplexidade do modelo no conjunto de teste.
        :param test_tokens: Lista de tokens do conjunto de teste.
        :return: Valor da perplexidade.
        """
        log_prob_sum = 0.0
        for i in range(1, len(test_tokens)):
            w1 = test_tokens[i - 1]
            w2 = test_tokens[i]
            prob = self.prob_matrix.get(w1, {}).get(w2, 1e-10)  # Evita log(0) com probabilidade mínima
            log_prob_sum += math.log2(prob)
        perplexity = 2 ** (-log_prob_sum / len(test_tokens))
        return perplexity

    def generate_text(self, length=20, max_punctuation_repeat=2):
        """
        Gera uma sequência de texto a partir do modelo de bigrama com aleatoriedade controlada.
        :param length: Número de tokens a serem gerados.
        :param max_punctuation_repeat: Limite de repetições consecutivas de pontuação.
        :return: String contendo o texto gerado.
        """
        current_token = self.tokenizer.encode("<|startoftext|>", allowed_special={'<|startoftext|>'})[0]
        generated_text = [current_token]
        punctuation_count = 0

        for _ in range(length - 1):
            next_tokens = self.prob_matrix.get(current_token, {})
            if not next_tokens:
                break

            # Seleciona o próximo token de forma probabilística
            next_token = random.choices(list(next_tokens.keys()), weights=next_tokens.values(), k=1)[0]

            # Controle de repetições de pontuação
            if self.tokenizer.decode([next_token]).strip() in [",", ".", "!", "?"]:
                punctuation_count += 1
                if punctuation_count > max_punctuation_repeat:
                    continue  # Ignora o token de pontuação extra
            else:
                punctuation_count = 0  # Reseta o contador se o próximo token não for pontuação

            generated_text.append(next_token)
            current_token = next_token

            # Para a geração ao encontrar o token de fim de texto
            if current_token == self.tokenizer.encode("<|endoftext|>", allowed_special={'<|endoftext|>'})[0]:
                break

        # Decodifica os tokens de volta para texto
        return self.tokenizer.decode(generated_text)